import os
import re

configfile:"config.yaml"

input_dir = config["input_dir"]
fasta = config["fasta"]
bed = config["bed"]
ehdn_script = config["EHDN_script"]
hip_script = config["HipSTR_script"]
split_bed = config["split_bed"]
trtools = config["trtools"]
stretch_env = config ["stretch_env"]
decoy_script = config["decoy_script"]
sort_script = config["sort_script"]
config_file= config["config_file"]
stretch_pipe = config["stretch_pipe"]

samples = [
    re.sub(r"\.cram$", "", file)
    for file in os.listdir(input_dir)
    if file.endswith(".cram")
]

bed_files = [f"results/HipSTR_prep/bed/splitbed_{i}.bed" for i in range(split_bed)]

rule all:
    input:
        "results/dataset_EHdn.tsv",
        "results/dataset_HipSTR.vcf",
        expand("results/dataset_GangSTR_{sample}.vcf",sample=samples),
        "temp.txt",
        #expand("results/STRetch/{sample}.STRs.tsv",sample=samples),
        #"results/STRetch/135556.STRs.tsv"


##ExpansionHunter Denovo
# Creates STR-profile per sample, containing info on STR longer than the read length of sample
rule EHdn_STR_profile:
    input:
        lambda wildcards: f"{input_dir}/{wildcards.sample}.cram"
    output:
        "results/EHdn_prep/str-profiles/{sample}.str_profile.json",
        temp("results/EHdn_prep/str-profiles/{sample}.locus.tsv"),
        temp("results/EHdn_prep/str-profiles/{sample}.motif.tsv")
    shell: "mkdir -p results && \
        ExpansionHunterDenovo profile \
        --reads {input} \
        --reference {fasta} \
        --output-prefix results/EHdn_prep/str-profiles/{wildcards.sample} \
        --min-anchor-mapq 50 \
        --max-irr-mapq 40"
        
# Creates tsv file containing sample id, case status, and path to the associated STR-profile
rule EHdn_manifest:
    input:
        expand("results/EHdn_prep/str-profiles/{sample}.str_profile.json",sample=samples)
    output:
        "results/EHdn_prep/manifest.tsv"
    script:
        "manifest.sh"
    
# Merges STR-profiles of all samples in one file
rule Ehdn_merge_profiles:
    input:
        "results/EHdn_prep/manifest.tsv"
    output:
        "results/EHdn_prep/dataset.multisample_profile.json"
    shell:"ExpansionHunterDenovo merge \
        --reference {fasta} \
        --manifest {input} \
        --output-prefix results/EHdn_prep/dataset"

# Locus-based outlier analysis
rule ExpansionHunterDenovo:
    input:
        manifest="results/EHdn_prep/manifest.tsv",
        data="results/EHdn_prep/dataset.multisample_profile.json"
    output:
        "results/dataset_EHdn.tsv"
    shell:"{ehdn_script} locus \
        --manifest {input.manifest} \
        --multisample-profile {input.data} \
        --output {output}" 


##HipSTR 
#Divides BED file in smaller files
rule HipSTR_divide_BED:
    input:
        bed
    output:
        bed_files
    shell: "mkdir -p results/HipSTR_prep/bed && split -d \
        --suffix-length=1 \
        --additional-suffix=.bed \
        -n l/{split_bed} {input} results/HipSTR_prep/bed/splitbed_"

#Makes a text file containing location of samples
rule sample_list:
    input:
        expand("{dir}/{sample}.cram", dir=input_dir, sample=samples)
    output:
        "results/HipSTR_prep/sample_files.txt"
    shell: "ls {input} > {output}"

#STR calling with de novo allele generation        
rule HipSTR:
    input:
        textfile="results/HipSTR_prep/sample_files.txt",
        bed="results/HipSTR_prep/bed/splitbed_{num}.bed"
    output:
        "results/HipSTR_prep/hipstr_{num}.vcf.gz"
    conda:
        "envs/HipSTR_filter.yaml"
    # add --def-stutter-model when sample amount <20 and --min-reads when sample amount <10
    shell: "HipSTR --bam-files {input.textfile} \
        --fasta {fasta} \
        --regions {input.bed} \
        --str-vcf {output} \
        --def-stutter-model \
        --min-reads 15 \
        --quiet"
       
#Filter HipSTR results
rule HipSTR_filter:
    input:
        "results/HipSTR_prep/hipstr_{num}.vcf.gz"
    output:
        "results/HipSTR_prep/filtered_hipstr_{num}.vcf"
    conda:
        "envs/HipSTR_filter.yaml"
    shell: "python {hip_script} \
        --vcf {input} \
        --min-call-qual         0.9 \
        --max-call-flank-indel  0.15 \
        --max-call-stutter      0.15 \
	    --min-call-allele-bias  -2 \
	    --min-call-strand-bias  -2 > {output}"

#Merge filtered HipSTR files
rule HipSTR_merge:
    input:
       expand("results/HipSTR_prep/filtered_hipstr_{num}.vcf",num=range(split_bed))
    output:
       "results/dataset_HipSTR.vcf"
    shell:"ls results/HipSTR_prep/filtered* > results/HipSTR_prep/filtered_hip.txt && \
        bcftools concat -f results/HipSTR_prep/filtered_hip.txt \
        -o {output}"


##GangSTR
rule GangSTR:
    input:
        samp=lambda wildcards: f"{input_dir}/{wildcards.sample}.cram",
        bed="results/HipSTR_prep/bed/splitbed_{num}.bed"
    output:
        "results/GangSTR_prep/gangstr_{sample}_{num}.vcf",
        temp("results/GangSTR_prep/gangstr_{sample}_{num}.insdata.tab"),
        temp("results/GangSTR_prep/gangstr_{sample}_{num}.samplestats.tab")
    shell: "mkdir -p results/GangSTR_prep && GangSTR --bam {input.samp} \
        --quiet \
        --ref {fasta} \
        --regions {input.bed} \
        --out results/GangSTR_prep/gangstr_{wildcards.sample}_{wildcards.num}"

#Filter GangSTR results
rule GangSTR_filter:
    input:
        "results/GangSTR_prep/gangstr_{sample}_{num}.vcf"
    output:
        "results/GangSTR_prep/filtered_gangstr_{sample}_{num}.vcf",
        temp("results/GangSTR_prep/filtered_gangstr_{sample}_{num}.samplog.tab"),
        temp("results/GangSTR_prep/filtered_gangstr_{sample}_{num}.loclog.tab")
    shell: "PYTHONPATH={trtools} \
        python -m trtools.dumpSTR.dumpSTR \
        --vcf {input} \
        --vcftype gangstr \
        --gangstr-max-call-DP 1000 \
        --gangstr-filter-spanbound-only \
        --gangstr-filter-badCI \
        --gangstr-min-call-DP 20 \
        --drop-filtered \
        --out results/GangSTR_prep/filtered_gangstr_{wildcards.sample}_{wildcards.num}"	      

#Merge GangSTR files per sample
rule GangSTR_merge:
    input:
       lambda wildcards: expand("results/GangSTR_prep/filtered_gangstr_{sample}_{num}.vcf",sample=wildcards.sample, num=range(split_bed))
    output:
       "results/dataset_GangSTR_{sample}.vcf"
    shell:"ls results/GangSTR_prep/filtered_gangstr_{wildcards.sample}*  > results/GangSTR_prep/filtered_gang.txt && \
        bcftools concat -f results/GangSTR_prep/filtered_gang.txt \
        -o {output}"

##STRetch
#Create STR decoy reference genome
rule decoy_ref:
    input:
        fasta
    output:
        "results/STRetch_prep/ref_STRdecoys_sorted.fasta"
    conda:
        stretch_env
    shell: "mkdir -p results/STRetch_prep && \
    python {decoy_script}  --length 2000 > results/STRetch_prep/STRdecoys.fasta && \
    cat {input} results/STRetch_prep/STRdecoys.fasta > results/STRetch_prep/ref_STRdecoys.fasta && \
    python {sort_script} --infile results/STRetch_prep/ref_STRdecoys.fasta --outfile {output}"

#Create extra indices required for STRetch pipeline
rule index:
    input:
        "results/STRetch_prep/ref_STRdecoys_sorted.fasta"
    output:
        "results/STRetch_prep/ref_STRdecoys_sorted.fasta.genome"
    conda:
        stretch_env
    shell: "bwa index {input} && samtools faidx {input} | \
    grep STR results/STRetch_prep/ref_STRdecoys_sorted.fasta.fai | \
    awk -v OFS='\t' '{{ print $1, 0, $2 -1 }}' > results/STRetch_prep/STRdecoys.bed | \
    bedtools sort -i results/STRetch_prep/STRdecoys.bed > results/STRetch_prep/STRdecoys_sorted.bed && \
    cut -f1,2  results/STRetch_prep/ref_STRdecoys_sorted.fasta.fai > {output}"

#Edit pipeline_config.groovy to point to new fasta and bed files
rule update_pipeline_config:
    input:
        config_file,
        "results/STRetch_prep/ref_STRdecoys_sorted.fasta.genome",
        decoybed="results/STRetch_prep/STRdecoys.bed",
        strfasta="results/STRetch_prep/ref_STRdecoys_sorted.fasta"
    output:
       temp("temp.txt")
    #CRAM_REf only needs to be replaced when using CRAM files
    shell:r"""sed -i "s|^CRAM_REF=.*|CRAM_REF=\"$(realpath {fasta})\"|" {config_file} && \
    sed -i "s|^REF=.*|REF=\"$(realpath {input.strfasta})\"|" {config_file} && \
    sed -i "s|^STR_BED=.*|STR_BED=\"$(realpath {bed})\"|" {config_file} && \
    sed -i "s|^DECOY_BED=.*|DECOY_BED=\"$(realpath {input.decoybed})\"|" {config_file} && \
    touch {output}"""

###This rule hasn't been completed once because it takes too long to analyse one sample
###Creates a lot of intermediate bam files (e.g 135556.10.STRdecoy.0000.bam)
#rule STRetch:
#    input:
#       "temp.txt",
#      ##samp=lambda wildcards: f"{input_dir}/{wildcards.sample}.cram"
#    output:
#       "results/STRetch/135556.STRs.tsv"
#       ##"results/STRetch/{sample}.STRs.tsv"
#    shell:"mkdir -p results/STRetch && \
#           bpipe run -p bwa_parallelism=24 -d results/STRetch {stretch_pipe} sample_data/135556.cram"
#         ##bpipe run -p bwa_parallelism=24  -d results/STRetch {stretch_pipe} {input.samp}"
