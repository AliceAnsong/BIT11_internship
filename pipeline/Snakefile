import os
import re

configfile:"config.yaml"

input_dir = config["input_dir"]
fasta = config["fasta"]
bed = config["bed"]
ehdn_script = config["EHDN_script"]
hip_script = config["HipSTR_script"]
split_bed = config["split_bed"]

samples = [
    re.sub(r"\.cram$", "", file)
    for file in os.listdir(input_dir)
    if file.endswith(".cram")
]

bed_files = [f"results/HipSTR_prep/bed/splitbed_{i}.bed" for i in range(split_bed)]

rule all:
    input:
        "results/dataset_EHdn.tsv",
        expand("results/HipSTR_prep/hipstr_{num}.vcf.gz", num=range(split_bed))


###ExpansionHunter Denovo
# Creates STR-profile per sample, containing info on STR longer than the read length of sample
rule EHdn_STR_profile:
    input:
        lambda wildcards: f"{input_dir}/{wildcards.sample}.cram"
    output:
        "results/EHdn_prep/str-profiles/{sample}.str_profile.json"
    params:
        fasta=fasta
    shell: "mkdir -p results && \
        ExpansionHunterDenovo profile \
        --reads {input} \
        --reference {params.fasta} \
        --output-prefix results/EHdn_prep/str-profiles/{wildcards.sample} \
        --min-anchor-mapq 50 \
        --max-irr-mapq 40 "
        
# Creates tsv file containing sample id, case status, and path to the associated STR-profile
rule EHdn_manifest:
    input:
        expand("results/EHdn_prep/str-profiles/{sample}.str_profile.json",sample=samples)
    output:
        "results/EHdn_prep/manifest.tsv"
    script:
        "manifest.sh"
    
# Merges STR-profiles of all samples in one file
rule Ehdn_merge_profiles:
    input:
        expand("results/EHdn_prep/str-profiles/{sample}.str_profile.json", sample=samples),
        "results/EHdn_prep/manifest.tsv"
    output:
        "results/EHdn_prep/dataset.multisample_profile.json"
    params:
        fasta=fasta
    shell:"ExpansionHunterDenovo merge \
        --reference {params.fasta} \
        --manifest results/EHdn_prep/manifest.tsv \
        --output-prefix results/EHdn_prep/dataset"

# Motif-based outlier analysis
rule ExpansionHunterDenovo:
    input:
        "results/EHdn_prep/manifest.tsv",
        "results/EHdn_prep/dataset.multisample_profile.json"
    output:
        "results/dataset_EHdn.tsv"
    shell:"{ehdn_script}/outlier.py motif \
        --manifest results/EHdn_prep/manifest.tsv \
        --multisample-profile results/EHdn_prep/dataset.multisample_profile.json \
        --output results/dataset_EHdn.tsv" \


###HipSTR 
rule HipSTR_divide_BED:
    input:
        bed
    output:
        bed_files
    shell: "mkdir -p results/HipSTR_prep/bed && split -d \
        --suffix-length=1 \
        --additional-suffix=.bed \
        -n l/{split_bed} {input} results/HipSTR_prep/bed/splitbed_"
        
rule HipSTR:
    input:
        expand("{dir}/{sample}.cram", dir=input_dir, sample=samples),
        bed="results/HipSTR_prep/bed/splitbed_{num}.bed"
    output:
        "results/HipSTR_prep/hipstr_{num}.vcf.gz"
    # add --def-stutter-model when sample amount <20 and --min-reads when samples are few
    shell: "ls {input_dir}/*.cram > {input_dir}/sample_files.txt | \
        HipSTR --bam-files {input_dir}/sample_files.txt \
        --fasta {fasta} \
        --regions {input.bed} \
        --str-vcf {output} \
        --def-stutter-model \
        --min-reads 15"
#
# Filter HipSTR results
#rule HipSTR_filter:
#    input:
#        "results/hipstr.vcf.gz"
#    output:
#        "results/hipstr.vcf.gz"
#    conda:
#        "env/HipSTR_filter.yaml"
#    shell: "python {hip_script} \
#        --vcf results/hipstr.vcf.gz \
#        --min-call-qual         0.9 \
#        --max-call-flank-indel  0.15 \
#        --max-call-stutter      0.15 \
#		--min-call-allele-bias  -2 \
#		--min-call-strand-bias  -2"
